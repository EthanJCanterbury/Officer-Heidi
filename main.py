import os
import re
import git
import requests
import json
from datetime import datetime, timedelta
from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler
from flask import Flask, request, jsonify
import tempfile
import shutil
import subprocess
from collections import defaultdict
import threading

# Initialize Slack app
app = App(token=os.environ.get("SLACK_BOT_TOKEN"))

# Initialize Flask app for webhook
flask_app = Flask(__name__)

class CodeAnalyzer:
    def __init__(self):
        self.ai_indicators = [
            # Direct AI attribution
            r'# This code was generated by',
            r'// Generated by',
            r'""".*AI.*generated.*"""',
            r'# AI-generated',
            r'// AI-generated',
            r'Created by ChatGPT',
            r'Generated by GPT',
            r'AI Assistant',
            r'Claude generated',
            r'Copilot suggestion',

            # Emoji usage in code (very suspicious)
            r'[ðŸš€ðŸ”¥ðŸ’¡â­ï¸âœ¨ðŸŽ¯ðŸ“ðŸŒŸðŸ’¯ðŸ”§âš¡ï¸ðŸŽ‰ðŸŽŠðŸŽˆðŸŽðŸŒˆðŸ¦„ðŸ”®ðŸ’ŽðŸ†ðŸ…ðŸŽ–ï¸ðŸ¥‡ðŸ¥ˆðŸ¥‰ðŸ´â€â˜ ï¸]',

            # Overly verbose/explanatory comments
            r'# This function does exactly what you think it does',
            r'# Initialize the variable',
            r'# Return the result',
            r'# TODO: Add error handling',
            r'# Note: This could be optimized',
            r'# Helper function',
            r'# Main function',
            r'# Import necessary libraries',
            r'# Set up the.*',
            r'# Create.*instance',
            r'# Process.*data',
            r'# Handle.*error',

            # Perfect step-by-step comments
            r'# Step \d+:',
            r'# First,',
            r'# Second,',
            r'# Finally,',
            r'# Next,',

            # Overly formal language
            r'# Utilize',
            r'# Implement',
            r'# Instantiate',
            r'# Subsequently',
            r'# Furthermore',
            r'# Nevertheless',

            # Perfect variable naming patterns
            r'result_\w+',
            r'final_\w+',
            r'temp_\w+',
            r'current_\w+',

            # Excessive type hints (Python)
            r'def \w+\([^)]*\) -> \w+:',

            # Perfect exception handling
            r'except Exception as e:',
            r'except \w+Error:',

            # Overly structured imports
            r'from typing import.*',
            r'import os, sys',

            # AI-like function names
            r'def process_.*\(',
            r'def handle_.*\(',
            r'def calculate_.*\(',
            r'def generate_.*\(',
            r'def create_.*\(',

            # Perfect docstrings
            r'""".*Args:.*Returns:.*"""',
            r'""".*Parameters:.*Returns:.*"""',

            # Suspicious print statements
            r'print\(f["\'].*{.*}.*["\'].*\)',
            r'print\(["\'].*successful.*["\'].*\)',
            r'print\(["\'].*completed.*["\'].*\)',

            # Perfect error messages
            r'["\']Error:.*["\']',
            r'["\']Warning:.*["\']',
            r'["\']Success:.*["\']',
        ]

        self.comment_patterns = [
            r'#.*',  # Python comments
            r'//.*',  # Single line comments
            r'/\*.*?\*/',  # Multi-line comments
            r'""".*?"""',  # Python docstrings
            r"'''.*?'''",  # Python docstrings
        ]

        self.suspicious_filenames = [
            r'generated',
            r'helper',
            r'utility',
            r'processed',
            r'tmp_',
            r'new_',
            r'updated_',
            r'data_',
            r'ai_',
            r'gpt_',
            r'function',
        ]

    def analyze_file(self, file_path, content):
        """Analyze a single file for AI indicators and comment ratio"""
        if not content.strip():
            return {"comments": 0, "code_lines": 0, "ai_score": 0}

        lines = content.split('\n')
        comment_lines = 0
        code_lines = 0
        ai_score = 0

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Check for comments
            is_comment = False
            for pattern in self.comment_patterns:
                if re.search(pattern, line):
                    comment_lines += 1
                    is_comment = True
                    break

            if not is_comment and line:
                code_lines += 1

            # Check for AI indicators with weighted scoring
            for pattern in self.ai_indicators:
                if re.search(pattern, line, re.IGNORECASE):
                    # Weight different patterns differently
                    if any(emoji in line for emoji in ['ðŸš€', 'ðŸ”¥', 'ðŸ’¡', 'â­', 'âœ¨', 'ðŸŽ¯', 'ðŸ“', 'ðŸŒŸ', 'ðŸ’¯', 'ðŸ”§', 'âš¡', 'ðŸŽ‰']):
                        ai_score += 25  # Emojis in code are highly suspicious
                    elif 'generated' in line.lower() or 'ai' in line.lower():
                        ai_score += 30  # Direct AI attribution
                    elif re.search(r'# Step \d+:', line):
                        ai_score += 15  # Step-by-step comments
                    elif re.search(r'def (process|handle|calculate|generate|create)_', line):
                        ai_score += 8   # AI-like function names
                    else:
                        ai_score += 5   # General AI indicators

        # Advanced pattern analysis
        if code_lines > 0:
            comment_ratio = comment_lines / code_lines

            # Suspicious comment ratios
            if comment_ratio > 0.4:  # Extremely high comment ratio
                ai_score += 15
            elif comment_ratio > 0.25:  # High comment ratio
                ai_score += 8

            # Check for perfect formatting patterns
            if self._has_perfect_formatting(content):
                ai_score += 20

            # Check for overly structured code
            if self._is_overly_structured(content):
                ai_score += 15

            # Check for suspicious naming patterns
            if self._has_ai_naming_patterns(content):
                ai_score += 12

            # Check for excessive error handling
            if self._has_excessive_error_handling(content):
                ai_score += 10

            # Check for AI-like imports
            if self._has_ai_imports(content):
                ai_score += 8

        return {
            "comments": comment_lines,
            "code_lines": code_lines,
            "ai_score": min(ai_score, 100)
        }

    def _is_overly_structured(self, content):
        """Check if code is overly structured (AI indicator)"""
        lines = content.split('\n')
        total_lines = len(lines)

        if total_lines == 0:
            return False

        # Check for excessive function definitions
        func_count = len(re.findall(r'def\s+\w+\(', content))
        if total_lines > 0 and func_count / total_lines > 0.1:
            return True

        # Check for excessive try-catch blocks
        try_count = len(re.findall(r'try:', content))
        if try_count > 3:
            return True

        # Check for excessive if-elif chains
        elif_count = len(re.findall(r'elif\s+', content))
        if elif_count > 5:
            return True

        return False

    def _has_perfect_formatting(self, content):
        """Check for suspiciously perfect formatting"""
        lines = content.split('\n')

        # Check for consistent indentation (too perfect)
        indent_pattern = re.compile(r'^(\s*)')
        indents = []
        for line in lines:
            if line.strip():
                match = indent_pattern.match(line)
                if match:
                    indents.append(len(match.group(1)))

        # If all indentations are multiples of 4, it's suspicious
        if len(indents) > 10:
            perfect_indents = sum(1 for i in indents if i % 4 == 0)
            if perfect_indents / len(indents) > 0.9:
                return True

        # Check for perfect spacing around operators
        operators = ['+', '-', '*', '/', '=', '==', '!=', '<=', '>=']
        perfect_spacing = 0
        total_operators = 0

        for line in lines:
            for op in operators:
                if op in line:
                    total_operators += line.count(op)
                    perfect_spacing += len(re.findall(f' {re.escape(op)} ', line))

        if total_operators > 5 and perfect_spacing / total_operators > 0.8:
            return True

        return False

    def _has_ai_naming_patterns(self, content):
        """Check for AI-typical variable and function naming"""
        # AI often uses very descriptive, long variable names
        long_vars = re.findall(r'\b[a-z][a-z_]{10,}\b', content)
        if len(long_vars) > 5:
            return True

        # Check for overly descriptive function names
        descriptive_funcs = re.findall(r'def [a-z][a-z_]{15,}\(', content)
        if len(descriptive_funcs) > 2:
            return True

        # Check for temporary variable patterns
        temp_vars = len(re.findall(r'\b(temp|tmp|result|output|data|info|item)_\w+', content))
        if temp_vars > 3:
            return True

        return False

    def _has_excessive_error_handling(self, content):
        """Check for overly comprehensive error handling"""
        try_blocks = len(re.findall(r'try:', content))
        except_blocks = len(re.findall(r'except', content))

        lines = len(content.split('\n'))
        if lines > 0:
            error_ratio = (try_blocks + except_blocks) / lines
            if error_ratio > 0.1:  # More than 10% of lines are error handling
                return True

        # Check for generic exception catching
        generic_except = len(re.findall(r'except Exception', content))
        if generic_except > 2:
            return True

        return False

    def _has_ai_imports(self, content):
        """Check for AI-typical import patterns"""
        # Check for importing everything from typing
        if re.search(r'from typing import.*,.*,.*', content):
            return True

        # Check for overly organized imports
        import_lines = [line for line in content.split('\n') if line.strip().startswith(('import ', 'from '))]
        if len(import_lines) > 8:
            # Check if imports are perfectly organized (stdlib, third-party, local)
            stdlib_imports = [line for line in import_lines if any(lib in line for lib in ['os', 'sys', 're', 'json', 'datetime'])]
            if len(stdlib_imports) > 3 and import_lines[0:len(stdlib_imports)] == stdlib_imports:
                return True

        return False

    def analyze_file_metadata(self, file_path, content):
        """Analyze file metadata for suspicious patterns"""
        suspicious_score = 0

        # Check for suspicious filenames
        filename = os.path.basename(file_path)
        for pattern in self.suspicious_filenames:
            if re.search(pattern, filename, re.IGNORECASE):
                suspicious_score += 10
                break

        # Check file size (AI often generates medium-sized files)
        file_size = len(content)
        if 500 < file_size < 5000:  # Sweet spot for AI-generated code
            suspicious_score += 5
        elif file_size < 100:  # Very small files are suspicious
            suspicious_score += 8

        # Check for round numbers in file size (AI tends to generate predictable lengths)
        if file_size % 100 == 0 and file_size > 500:
            suspicious_score += 12

        return suspicious_score

    def analyze_commit_patterns(self, commits):
        """Analyze commit patterns for AI indicators"""
        if not commits:
            return 0

        suspicious_score = 0
        commit_messages = [commit['message'] for commit in commits]

        # Check for generic commit messages
        generic_patterns = [
            r'^(initial commit|first commit|add .*|update .*|fix .*|refactor .*)$',
            r'^(Added|Updated|Fixed|Refactored) ',
            r'^(feat|fix|docs|style|refactor|test|chore): ',
            r'^\w+ \w+$',  # Two-word commits
        ]

        generic_commits = 0
        for message in commit_messages:
            for pattern in generic_patterns:
                if re.search(pattern, message.strip(), re.IGNORECASE):
                    generic_commits += 1
                    break

        if len(commit_messages) > 0:
            generic_ratio = generic_commits / len(commit_messages)
            if generic_ratio > 0.7:
                suspicious_score += 20
            elif generic_ratio > 0.5:
                suspicious_score += 10

        # Check for suspicious commit timing (all commits within short timeframe)
        if len(commits) > 3:
            dates = []
            for commit in commits:
                try:
                    date = datetime.strptime(commit['date'], '%Y-%m-%d %H:%M:%S')
                    dates.append(date)
                except:
                    continue

            if len(dates) > 1:
                time_span = (max(dates) - min(dates)).total_seconds() / 3600  # hours
                commits_per_hour = len(dates) / max(time_span, 0.1)

                if commits_per_hour > 5:  # More than 5 commits per hour
                    suspicious_score += 15
                elif time_span < 1 and len(dates) > 5:  # Many commits in less than 1 hour
                    suspicious_score += 25

        # Check for identical commit messages
        unique_messages = set(commit_messages)
        if len(commit_messages) > 0 and len(unique_messages) / len(commit_messages) < 0.8:
            suspicious_score += 15

        return min(suspicious_score, 50)

    def detect_code_duplication(self, file_contents):
        """Detect excessive code duplication across files"""
        if len(file_contents) < 2:
            return 0

        suspicious_score = 0

        # Simple line-based similarity check
        all_lines = []
        for content in file_contents:
            lines = [line.strip() for line in content.split('\n') if line.strip() and not line.strip().startswith('#')]
            all_lines.extend(lines)

        if len(all_lines) > 10:
            unique_lines = set(all_lines)
            duplication_ratio = 1 - (len(unique_lines) / len(all_lines))

            if duplication_ratio > 0.3:  # More than 30% duplication
                suspicious_score += 20
            elif duplication_ratio > 0.2:
                suspicious_score += 10

        return suspicious_score

    def get_ai_likelihood(self, score):
        """Convert numeric score to likelihood rating"""
        if score < 10:
            return "Not AI"
        elif score < 30:
            return "Maybe AI"
        elif score < 60:
            return "Probly AI"
        else:
            return "Definitly AI"

def clone_repository(repo_url, temp_dir):
    """Clone the GitHub repository"""
    try:
        repo = git.Repo.clone_from(repo_url, temp_dir)
        return repo
    except Exception as e:
        raise Exception(f"Failed to clone repository: {str(e)}")

def get_github_user_info(repo_url):
    """Get GitHub user information"""
    try:
        # Extract username from repo URL
        match = re.search(r'github\.com/([^/]+)', repo_url)
        if not match:
            return None

        username = match.group(1)

        # Get user info from GitHub API
        response = requests.get(f"https://api.github.com/users/{username}")
        if response.status_code == 200:
            user_data = response.json()
            created_at = datetime.strptime(user_data['created_at'], '%Y-%m-%dT%H:%M:%SZ')
            account_age = (datetime.now() - created_at).days

            return {
                'username': username,
                'created_at': user_data['created_at'],
                'account_age_days': account_age,
                'public_repos': user_data['public_repos'],
                'followers': user_data['followers']
            }
    except Exception as e:
        print(f"Error getting user info: {e}")

    return None

def analyze_commits(repo):
    """Analyze repository commits"""
    commits = []
    try:
        for commit in repo.iter_commits(max_count=50):  # Last 50 commits
            commits.append({
                'hash': commit.hexsha[:8],
                'message': commit.message.strip(),
                'author': commit.author.name,
                'date': commit.committed_datetime.strftime('%Y-%m-%d %H:%M:%S'),
                'files_changed': len(commit.diff('HEAD~1').iter_change_type('M'))
            })
    except:
        pass

    return commits

def should_analyze_file(file_path):
    """Check if file should be analyzed"""
    # Skip files starting with .
    if os.path.basename(file_path).startswith('.'):
        return False

    # Skip package manager files
    package_files = [
        'package.json', 'package-lock.json', 'yarn.lock',
        'requirements.txt', 'pyproject.toml', 'setup.py',
        'Cargo.toml', 'Cargo.lock', 'go.mod', 'go.sum'
    ]

    if os.path.basename(file_path) in package_files:
        return False

    # Only analyze code files
    code_extensions = ['.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.cs', '.rb', '.go', '.rs', '.php']
    return any(file_path.endswith(ext) for ext in code_extensions)

@app.command("/heidi-check")
def heidi_check(ack, respond, command):
    ack()

    text = command['text'].strip()

    # Check for -p flag
    public_response = False
    if text.startswith('-p '):
        public_response = True
        repo_url = text[3:].strip()
    else:
        repo_url = text

    if not repo_url:
        respond("Please provide a GitHub repository URL. Usage: `/heidi-check [-p] https://github.com/user/repo`\nUse `-p` flag to post results publicly.")
        return

    respond("ðŸ” Officer Heidi is analyzing the repository... This may take a moment.")

    temp_dir = None
    try:
        # Create temporary directory
        temp_dir = tempfile.mkdtemp()

        # Clone repository
        repo = clone_repository(repo_url, temp_dir)

        # Get user info
        user_info = get_github_user_info(repo_url)

        # Analyze commits
        commits = analyze_commits(repo)

        # Analyze code files
        analyzer = CodeAnalyzer()
        total_files = 0
        total_comments = 0
        total_code_lines = 0
        total_ai_score = 0
        file_results = []
        language_counts = defaultdict(int)
        file_contents = [] # For code duplication analysis

        for root, dirs, files in os.walk(temp_dir):
            for file in files:
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, temp_dir)

                if should_analyze_file(rel_path):
                    try:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()

                        result = analyzer.analyze_file(rel_path, content)

                        # File metadata analysis
                        metadata_score = analyzer.analyze_file_metadata(rel_path, content)
                        result['ai_score'] += metadata_score
                        result['ai_score'] = min(result['ai_score'], 100)

                        if result['code_lines'] > 0:
                            total_files += 1
                            total_comments += result['comments']
                            total_code_lines += result['code_lines']
                            total_ai_score += result['ai_score']
                            file_results.append((rel_path, result))
                            file_contents.append(content) # Collect file contents

                            # Determine language and increment count
                            language = "Unrecognized"
                            if rel_path.endswith('.py'):
                                language = "Python"
                            elif rel_path.endswith(('.js', '.ts')):
                                language = "JavaScript/TypeScript"
                            elif rel_path.endswith('.java'):
                                language = "Java"
                            elif rel_path.endswith(('.cpp', '.c', '.h')):
                                language = "C/C++"
                            elif rel_path.endswith('.cs'):
                                language = "C#"
                            elif rel_path.endswith('.rb'):
                                language = "Ruby"
                            elif rel_path.endswith('.go'):
                                language = "Go"
                            elif rel_path.endswith('.rs'):
                                language = "Rust"
                            elif rel_path.endswith('.php'):
                                language = "PHP"

                            language_counts[language] += 1
                    except:
                        continue

        # Analyze commit patterns
        commit_pattern_score = analyzer.analyze_commit_patterns(commits)
        total_ai_score += commit_pattern_score

        # Detect code duplication
        duplication_score = analyzer.detect_code_duplication(file_contents)
        total_ai_score += duplication_score

        # Calculate overall metrics
        if total_files > 0:
            avg_ai_score = total_ai_score / total_files
            comment_ratio = (total_comments / total_code_lines * 100) if total_code_lines > 0 else 0
            overall_likelihood = analyzer.get_ai_likelihood(avg_ai_score)
        else:
            avg_ai_score = 0
            comment_ratio = 0
            overall_likelihood = "Not AI"

        # Format response with proper Slack markdown
        response = "ðŸš” *Officer Heidi's Analysis Report*\n\n"
        response += f"*Repository:* {repo_url}\n"
        response += f"*Analysis Date:* {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        response += "*ðŸ‘¤ Repository Owner Info:*\n"

        if user_info:
            response += f"â€¢ Username: {user_info['username']}\n"
            response += f"â€¢ Account Age: {user_info['account_age_days']} days\n"
            response += f"â€¢ Account Created: {user_info['created_at'][:10]}\n"
            response += f"â€¢ Public Repos: {user_info['public_repos']}\n"
            response += f"â€¢ Followers: {user_info['followers']}\n"

            if user_info['account_age_days'] < 30:
                response += "âš ï¸ *WARNING: Very new GitHub account (< 30 days)*\n"
        else:
            response += "â€¢ Could not retrieve user information\n"

        response += f"\n*ðŸ“Š Code Analysis:*\n"
        response += f"â€¢ Files Analyzed: {total_files}\n"
        response += f"â€¢ Total Code Lines: {total_code_lines}\n"
        response += f"â€¢ Total Comment Lines: {total_comments}\n"
        response += f"â€¢ Comment-to-Code Ratio: {comment_ratio:.1f}%\n"

        response += f"\n*ðŸ”¤ Languages Detected:*\n"
        for language, count in sorted(language_counts.items()):
            response += f"â€¢ {language}: {count} file{'s' if count != 1 else ''}\n"

        response += f"\n*ðŸ¤– AI Detection Results:*\n"
        response += f"â€¢ Average AI Score: {avg_ai_score:.1f}/100\n"

        # Add color coding based on AI likelihood
        if overall_likelihood == "Definitly AI":
            response += f"â€¢ ðŸ”´ *AI Likelihood: {overall_likelihood}*\n"
        elif overall_likelihood == "Probly AI":
            response += f"â€¢ ðŸŸ  *AI Likelihood: {overall_likelihood}*\n"
        elif overall_likelihood == "Maybe AI":
            response += f"â€¢ ðŸŸ¡ *AI Likelihood: {overall_likelihood}*\n"
        else:
            response += f"â€¢ ðŸŸ¢ *AI Likelihood: {overall_likelihood}*\n"

        response += f"\n*ðŸ“ Recent Commits ({len(commits)}):*\n"
        for commit in commits[:10]:  # Show last 10 commits
            response += f"â€¢ `{commit['hash']}` - {commit['message'][:50]}{'...' if len(commit['message']) > 50 else ''} ({commit['date']})\n"

        if len(commits) > 10:
            response += f"... and {len(commits) - 10} more commits\n"

        # Add suspicious file analysis
        suspicious_files = [(path, result) for path, result in file_results if result['ai_score'] > 30]
        if suspicious_files:
            response += f"\n*âš ï¸ Suspicious Files (AI Score > 30):*\n"
            for path, result in suspicious_files[:5]:
                response += f"â€¢ `{path}` - AI Score: {result['ai_score']}\n"

        # Add detailed analysis for high scores
        if avg_ai_score > 50:
            response += f"\n*ðŸš¨ HIGH AI PROBABILITY DETECTED*\n"
            response += "Common AI indicators found:\n"
            response += "â€¢ Emoji usage in code comments\n"
            response += "â€¢ Perfect formatting and spacing\n"
            response += "â€¢ Overly descriptive variable names\n"
            response += "â€¢ Step-by-step comment patterns\n"
            response += "â€¢ Excessive error handling\n"

        respond(response, response_type="in_channel" if public_response else "ephemeral")

    except Exception as e:
        respond(f"âŒ Error analyzing repository: {str(e)}", response_type="in_channel" if public_response else "ephemeral")
    finally:
        if temp_dir and os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)

@flask_app.route('/webhook/analyze', methods=['POST'])
def analyze_repository_webhook():
    """Webhook endpoint for external services to analyze repositories"""
    try:
        data = request.get_json()
        
        if not data or 'repo_url' not in data:
            return jsonify({
                'error': 'Missing repo_url in request body',
                'status': 'error'
            }), 400
        
        repo_url = data['repo_url']
        
        # Validate GitHub URL
        if not repo_url.startswith('https://github.com/'):
            return jsonify({
                'error': 'Invalid GitHub repository URL',
                'status': 'error'
            }), 400
        
        print(f"ðŸ” Webhook analysis request for: {repo_url}")
        
        temp_dir = None
        try:
            # Create temporary directory
            temp_dir = tempfile.mkdtemp()
            
            # Clone repository
            repo = clone_repository(repo_url, temp_dir)
            
            # Get user info
            user_info = get_github_user_info(repo_url)
            
            # Analyze commits
            commits = analyze_commits(repo)
            
            # Analyze code files
            analyzer = CodeAnalyzer()
            total_files = 0
            total_comments = 0
            total_code_lines = 0
            total_ai_score = 0
            file_results = []
            language_counts = defaultdict(int)
            file_contents = []
            
            for root, dirs, files in os.walk(temp_dir):
                for file in files:
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, temp_dir)
                    
                    if should_analyze_file(rel_path):
                        try:
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                content = f.read()
                            
                            result = analyzer.analyze_file(rel_path, content)
                            
                            # File metadata analysis
                            metadata_score = analyzer.analyze_file_metadata(rel_path, content)
                            result['ai_score'] += metadata_score
                            result['ai_score'] = min(result['ai_score'], 100)
                            
                            if result['code_lines'] > 0:
                                total_files += 1
                                total_comments += result['comments']
                                total_code_lines += result['code_lines']
                                total_ai_score += result['ai_score']
                                file_results.append({
                                    'file_path': rel_path,
                                    'ai_score': result['ai_score'],
                                    'code_lines': result['code_lines'],
                                    'comment_lines': result['comments']
                                })
                                file_contents.append(content)
                                
                                # Determine language
                                language = "Unrecognized"
                                if rel_path.endswith('.py'):
                                    language = "Python"
                                elif rel_path.endswith(('.js', '.ts')):
                                    language = "JavaScript/TypeScript"
                                elif rel_path.endswith('.java'):
                                    language = "Java"
                                elif rel_path.endswith(('.cpp', '.c', '.h')):
                                    language = "C/C++"
                                elif rel_path.endswith('.cs'):
                                    language = "C#"
                                elif rel_path.endswith('.rb'):
                                    language = "Ruby"
                                elif rel_path.endswith('.go'):
                                    language = "Go"
                                elif rel_path.endswith('.rs'):
                                    language = "Rust"
                                elif rel_path.endswith('.php'):
                                    language = "PHP"
                                
                                language_counts[language] += 1
                        except:
                            continue
            
            # Analyze commit patterns
            commit_pattern_score = analyzer.analyze_commit_patterns(commits)
            total_ai_score += commit_pattern_score
            
            # Detect code duplication
            duplication_score = analyzer.detect_code_duplication(file_contents)
            total_ai_score += duplication_score
            
            # Calculate overall metrics
            if total_files > 0:
                avg_ai_score = total_ai_score / total_files
                comment_ratio = (total_comments / total_code_lines * 100) if total_code_lines > 0 else 0
                overall_likelihood = analyzer.get_ai_likelihood(avg_ai_score)
            else:
                avg_ai_score = 0
                comment_ratio = 0
                overall_likelihood = "Not AI"
            
            # Prepare response
            response_data = {
                'status': 'success',
                'repository_url': repo_url,
                'analysis_timestamp': datetime.now().isoformat(),
                'owner_info': user_info,
                'code_analysis': {
                    'files_analyzed': total_files,
                    'total_code_lines': total_code_lines,
                    'total_comment_lines': total_comments,
                    'comment_to_code_ratio': round(comment_ratio, 2),
                    'languages_detected': dict(language_counts)
                },
                'ai_detection': {
                    'average_ai_score': round(avg_ai_score, 2),
                    'ai_likelihood': overall_likelihood,
                    'commit_pattern_score': commit_pattern_score,
                    'code_duplication_score': duplication_score
                },
                'commits': commits[:10],  # Last 10 commits
                'suspicious_files': [
                    file_data for file_data in file_results 
                    if file_data['ai_score'] > 30
                ][:10]  # Top 10 suspicious files
            }
            
            return jsonify(response_data), 200
            
        except Exception as e:
            return jsonify({
                'error': f'Analysis failed: {str(e)}',
                'status': 'error'
            }), 500
        finally:
            if temp_dir and os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
                
    except Exception as e:
        return jsonify({
            'error': f'Request processing failed: {str(e)}',
            'status': 'error'
        }), 500

@flask_app.route('/webhook/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'service': 'Officer Heidi Analysis Service',
        'timestamp': datetime.now().isoformat()
    }), 200

@flask_app.route('/webhook/info', methods=['GET'])
def service_info():
    """Service information endpoint"""
    return jsonify({
        'service': 'Officer Heidi - AI Code Detection Service',
        'version': '1.0.0',
        'description': 'Webhook service for analyzing GitHub repositories for AI-generated code',
        'endpoints': {
            'analyze': {
                'method': 'POST',
                'url': '/webhook/analyze',
                'description': 'Analyze a GitHub repository for AI-generated code',
                'payload': {
                    'repo_url': 'https://github.com/username/repository'
                }
            },
            'health': {
                'method': 'GET',
                'url': '/webhook/health',
                'description': 'Health check endpoint'
            },
            'info': {
                'method': 'GET',
                'url': '/webhook/info',
                'description': 'Service information'
            }
        }
    }), 200

def run_flask_app():
    """Run Flask webhook server"""
    flask_app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)

def run_slack_bot():
    """Run Slack bot"""
    handler = SocketModeHandler(app, os.environ["SLACK_APP_TOKEN"])
    print("ðŸš” Officer Heidi Slack bot is on duty!")
    handler.start()

if __name__ == "__main__":
    print("ðŸš” Officer Heidi is starting up...")
    print("ðŸ“¡ Starting webhook server on port 5000...")
    print("ðŸ¤– Starting Slack bot...")
    
    # Start Flask webhook server in a separate thread
    flask_thread = threading.Thread(target=run_flask_app, daemon=True)
    flask_thread.start()
    
    # Start Slack bot in main thread
    run_slack_bot()